---
title: "City Pop and Pop, the same despite being a world apart from one another?"
author: Ragna Nilsen
output: 
  flexdashboard::flex_dashboard:
    orientation: columns
    vertical_layout: fill
---

```{r setup, include=FALSE}
library(tidyverse)
library(tidymodels)
library(ggdendro)
library(spotifyr)
library(ggplot2)
library(flexdashboard)
library(plotly)
library(compmus)
  get_conf_mat <- function(fit) {
  outcome <- .get_tune_outcome_names(fit)
  fit |> 
    collect_predictions() |> 
    conf_mat(truth = outcome, estimate = .pred_class)
}  

get_pr <- function(fit) {
  fit |> 
    conf_mat_resampled() |> 
    group_by(Prediction) |> mutate(precision = Freq / sum(Freq)) |> 
    group_by(Truth) |> mutate(recall = Freq / sum(Freq)) |> 
    ungroup() |> filter(Prediction == Truth) |> 
    select(class = Prediction, precision, recall)
} 
library(knitr)
library(kknn)
library(heatmaply)
```

```{r load data, include=FALSE}
cityPopPlaylist <- get_playlist_audio_features("", "7LNMKu5boirXk30CB0mh7W")
popPlaylist <- get_playlist_audio_features("", "64h8THGlWTBkbE7sqhRV4V")
```

```{r bind data, include=FALSE}
comparison <- 
    bind_rows(
      cityPopPlaylist |> mutate(category = "City Pop"),
      popPlaylist |> mutate(category = "US Pop")
    )
```

Introduction
=======================================

Column {data-width=600}
--------------------------------------


City pop vs U.S. Pop   
    
In this portfolio, I will explore the differences and features of the two genres City Pop (CP) and U.S.Pop (UP), originating from Japan and the U.S. respectively. Genre here is a very loose term, as there are [sources](https://www.japantimes.co.jp/culture/2015/07/05/music/city-pop-revival-literally-trend-name/) that indicate that CP is a vibe explored through many different types of genres. [Others](https://www.rollingstone.com/music/music-features/japanese-city-pop-returns-light-in-the-attic-compilation-pacific-breeze-822663/) say it is pop music for city people. However, for simplicity's sake, I will use the term genre. CP is a type of genre from Japan that appeared the late 70's and reached its popularity peak in the 80's. I will be comparing two playlists consisting of three artists, one Japanese group and one US playlist. The Japanese group consist of the artists Taeko Onuki, Miki Matsubara and Anri. The US counterparts are the artists Michael Jackson, Whitney Houston and Madonna. I chose these corpora because I want to explore whether there are distinct differences between the genre of (city) pop as it was in Japan in the 80's vs the pop that was popular in the western world in the same decade. Japanese CP was influenced by western music, so I expect there to be many similarities in use of sound, instruments and type of rhythms. However, an aspect I am particularly interested whether there is a difference is the prevalence of bass, and tempo. It is also interesting to see whether there are differences in other aspects like timbre. However, I am unsure to what extent they are different, which will be explored here.  

The reason I chose this topic is because of my personal tastes. Ever since I was a child, pop has been a significant part of my life and upbringing as this was perhaps the major genre both my parents listened to. Due to rising popularity on the internet, CP has gained a lot of traction and even has spawned new types of sub-genres, e.g., future funk, from what I think is a similarity from Western pop as well as many of the songs ability to sound modern in today's standards. This previously mention of pop for city people is what attracted me, and I have been listening to it personally since I discovered it 3-4 years ago. 
    
As I have chosen three artists to represent their own (variety) of genres, there might be nuances and representations I am missing. Taeko Onuki, Miki Matsubara and Anri were chosen due to their popularity on Spotify (the amount of general listeners as well as listens to their tracks). I also have to mention that there were personal selections. Nevertheless, they were centered around three albums from each respective artist. The same method was done in choosing the western counterparts. However, the genre(s) is (are) very broad, despite its popularity, and some varieties might have been overlooked. However, their popularity is a strength as many causal listeners will have knowledge of these songs.   
  

Typical, and popular, tracks from the Japanese playlist are:  

  - [Mayonaka no Door / Stay With Me](https://open.spotify.com/track/2BHj31ufdEqVK5CkYDp9mA?si=6f913519033540d7) - Miki Matsubara from "Miki Matsubara Best Collection
  - [4:00A.M.](https://open.spotify.com/track/0zoGVO4bQXG8U6ChKwNgeg?si=e8ef83a75f054faa) - Taeko Onuki from "Mignonne"
  - [Remember Summer Days](https://open.spotify.com/track/1qUo7d5lAOclNVbTUY0A2R?si=7b1efc2b7d6d4653) - Anri from "Timely!!"

These songs are typical in the sense that there are prominent use of basslines and clear rhythms, very stereotypical pop and have many timbre features to them, as well as many sound layers, e.g., instruments etc.  

The western counterparts have typical tracks like:  

  - [Thriller]() - Michael Jackson from "Thriller" 
  - [I Wanna Dance with Somebody (Who Loves Me)](https://open.spotify.com/track/2tUBqZG2AbRi7Q0BIrVrEj?si=8565b305f0994f16) - Whitney Houston from     "Whitney"
  - [Material Girl](https://open.spotify.com/track/7bkyXSi4GtVfD7itZRUR3e?si=46d0d3fa86d942ea) - Madonna from "Like a Virgin"  
  
These last three tracks especially has the typical and distinct features of pop of the 80's, namely the sharp drums and the heavily synthesized piano sounds and, what I think, an almost like a "dreamy" sound to them.  
  
Atypical songs from both playlists can include: 

  - [Billie Jean](https://open.spotify.com/track/7J1uxwnxfQLu4APicE5Rnj?si=2204b32a10f545b5) by Michael Jackson
  - [横顔](https://open.spotify.com/track/5IgmcXoZGOhs32mD1BLFdp?si=fc1ffeeb007a46ad) by Taeko Onuki  
  
  
In order to explore eventual differences and features of this corpus, I will first start with a classification model with a random forest. Then I will explore track level features between the two genres, focusing on what the classification model labels my corpus. After I will go more into the musical moments such as timbre and chroma, focusing on self-similarity matrices and chromagrams and chordograms. Finally, I will go into depth about what this portfolio has explored and conclude what can be derived from this. 

Column {data-width=200}
--------------------------------------

### Playlist CP 
<iframe src="https://open.spotify.com/embed/playlist/7LNMKu5boirXk30CB0mh7W?utm_source=generator" width="100%" height="100%" frameBorder="0" allowtransparency="true" allow="encrypted-media" data-external="1"></iframe>

Column {data-width=200}
--------------------------------------

### Playlist UP 
<iframe src="https://open.spotify.com/embed/playlist/64h8THGlWTBkbE7sqhRV4V?utm_source=generator" width="100%" height="100%" frameBorder="0" allowtransparency="true" allow="encrypted-media" data-external="1"></iframe>


Classification {.storyboard}
======================================

### The classifier is better than expected at categorizing each genre
```{r}
cPop <- get_playlist_audio_features("spotify", "7LNMKu5boirXk30CB0mh7W")
uPop <- get_playlist_audio_features("spotify", "64h8THGlWTBkbE7sqhRV4V")
Pop <-
  bind_rows(
    cPop |> mutate(playlist = "City Pop") |> slice_head(n = 31),
    uPop |> mutate(playlist = "U.S. Pop") |> slice_head(n = 31),
  ) |> 
  add_audio_analysis()
```

```{r}
pop_features <-
  Pop |>  
  mutate(
    playlist = factor(playlist),
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(
        segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean",
      )
  ) |>
  mutate(pitches = map(pitches, compmus_normalise, "clr")) |>
  mutate_at(vars(pitches, timbre), map, bind_rows) |>
  unnest(cols = c(pitches, timbre))
```

```{r}
pop_recipe <-
  recipe(
    playlist ~
      danceability +
      energy +
      loudness +
      speechiness +
      acousticness +
      instrumentalness +
      liveness +
      valence +
      tempo +
      duration +
      C + `C#|Db` + D + `D#|Eb` +
      E + `F` + `F#|Gb` + G +
      `G#|Ab` + A + `A#|Bb` + B +
      c01 + c02 + c03 + c04 + c05 + c06 +
      c07 + c08 + c09 + c10 + c11 + c12,
    data = pop_features           # Use the same name as the previous block.
  ) |>
  step_center(all_predictors()) |>
  step_scale(all_predictors())      # Converts to z-scores.
  # step_range(all_predictors())    # Sets range to [0, 1].
```

```{r}
pop_cv <- pop_features |> vfold_cv(5)
```

```{r}
knn_model <-
  nearest_neighbor(neighbors = 1) |>
  set_mode("classification") |> 
  set_engine("kknn")
pop_knn <- 
  workflow() |> 
  add_recipe(pop_recipe) |> 
  add_model(knn_model) |> 
  fit_resamples(pop_cv, control = control_resamples(save_pred = TRUE))
```

```{r, include=FALSE}
pop_knn |> get_conf_mat()
```


```{r}
pop_knn |> get_conf_mat() |> autoplot(type = "heatmap")
```

*** 

```{r}
pop_knn |> get_pr()
```

  
In order to compute the model, I did capped the playlists at 31 songs in each group. The model stays around over 20 correctly predicted songs in each genre. By this, I can assume that the model is decent at classifying exactly what City Pop and what U.S Pop is. The prediction for both genres stay around 0.7-0.8 and accuracy stay around the same values. In the next header, I will see what kind of labels were the most important in the classification of these playlists, and determine what labels where most important in this classification.


### From these labels the track level feature *loudness*, and *timbre coefficients* are the most important labels when classifying CP and UP.


```{r}
forest_model <-
  rand_forest() |>
  set_mode("classification") |> 
  set_engine("ranger", importance = "impurity")
pop_forest <- 
  workflow() |> 
  add_recipe(pop_recipe) |> 
  add_model(forest_model) |> 
  fit_resamples(
    pop_cv, 
    control = control_resamples(save_pred = TRUE)
  )
```

```{r}
workflow() |> 
  add_recipe(pop_recipe) |> 
  add_model(forest_model) |> 
  fit(pop_features) |> 
  pluck("fit", "fit", "fit") |>
  ranger::importance() |> 
  enframe() |> 
  mutate(name = fct_reorder(name, value)) |> 
  ggplot(aes(name, value)) + 
  geom_col() + 
  coord_flip() +
  theme_minimal() +
  labs(x = NULL, y = "Importance")
```

***

```{r}
pop_forest |> get_pr()
```
  
  
```{r}
pop_forest |> get_conf_mat()
```

In the random forest model, the accuracy and predictions also stay around the same values as the knn-model. Indicating that both of the models are decent at classifying the genres. 

Regarding the feature selection, depending on the runs three different labels move around the three first places in the feature plot. These are the labels *loudness*, *c1* and *c11*. The importance of loudness and the timbre coefficient c1 is quite humorous as it was said in one of the lectures that this timbre vector is the rough equivalent to loudness. After those, primarily timbre features are the ones that are of the most importance. This is quite interesting, as I did mention at the very start of the course (without any knowledge of the terms of music) said that City Pop has a lot of layers to them. This was meant as "there is a lot going on" and it sounds different.  

This is probably why, according to the sources mentioned in the introduction, the genre CP is majorly based on *vibes* and *urban* feelings - rather than distinct features from other music moments. 


Graphs {.storyboard}
==========================================

### CP is louder than UP


```{r tempo and loudness}
comparison |>
  ggplot(
    aes(
      x = tempo, 
      y = loudness, 
      color = category,
      size = energy
      )
    ) + 
    geom_point(
       alpha = 0.2
    ) + 
    geom_jitter() + 
    facet_wrap(
      ~category
      ) +
    labs(
      title = "The effect of tempo on loudness between groups", 
      x = "Tempo (bpm)", 
      y = "Loudness",
      size = "Energy", 
      color = "Genre"
    )
```

*** 

Here one can see the temporal and power features between the two genres. As one can see, there is a definite preference for a higher volume in the City Pop group than the other gorup. This graph was made as a direct consequence of the labeling done in the previous section. As one can see, there is a distinct difference in loudness between the two genres, as it seems that while both of them *tend* to stay around the same beats per minute, i.e., 120-125. Which coincidentally corresponds well to the study by Moelants (2002), which says that humans seem to prefer this tempo.   

However, one can tell that there are many items that do not correspond to this as there are plots between the 90-120 area in CP and from 90-150 in UP. One could perhaps say that UP have more items directly corresponding to 120 bpm. 

Nevertheless, UP, in terms of loudness, stop at just above -9 dB. What is very clear is that CP does not stop at this as multiple items is above -6 dB. This, if I am interpreting it correctly, does mean that CP is more loud than UP. 


### Other track level features single out some outliers. 

```{r energyxdance, warning=FALSE}
comparison |>
  ggplot(
    aes(
      x = energy, 
      y = danceability, 
      color = category, 
      size = tempo
      )
    ) + 
    geom_point() + 
    geom_smooth() +
    facet_wrap(~category) +
    labs(
      title = "The effect of energy on danceability for City Pop and US Pop", 
      size = "Tempo", 
      x = "Energy", 
      y = "Danceability")
```

***

Here is a plot of the effect of energy on danceability, with size of the plots as well as the band around the line indicating the tempo of the songs. One can see that there is more of a linear trend with US Pop, indicating that up until around energy of 0.5 there seems to be a correlation between energy and danceability. Tempo, however, do seem to have no pattern at first glance. Whereas for the City Pop playlist, there seem to be a slight curve in the beginning of the graph, but overall there is a very even trend of the effect of energy on danceability. At first glance, there also seem to be no indication that there is a trend for tempo.  

However, once can tell that U.S. Pop seem to have more of a positive linear correlation between energy and danceability - more than its eastern counterpart in any case.

In both groups, there are two outliers that especially draw one's eye - which is "Billie Jean" by Michael Jackson, and "横顔" by Taeko Onuki. They both have, in comparison to other songs, very low energy while at the same time having high danceability. 


Chromagrams of Typicals
====================================

Column {data-width=650}
------------------------------------

### Chromagrams of 4:00A.M. and I Wanna Dance..
```{r}
fourSomebody <- 
    bind_rows(
      mutate(get_tidy_audio_analysis("2tUBqZG2AbRi7Q0BIrVrEj") |> select(segments) |> unnest(segments) |> select(start, duration, pitches), song = "I Wanna Dance.. - Whitney"), 
      mutate(get_tidy_audio_analysis("0zoGVO4bQXG8U6ChKwNgeg") |> select(segments) |> unnest(segments) |> select(start, duration, pitches), song = "4:00A.M. - Taeko Onuki")
    )

fourSomebody |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  facet_wrap(~song) +
  theme_minimal() +
  scale_fill_viridis_c()
```

Column {data-width=300}
-------------------------------------

### Analysis

Both "4:00A.M." and "I Wanna Dance with Somebody (Who Loves Me)" are both mentioned as popular and typical songs in my corpus. As depicted in the chromagrams to the left. As one can see. The same chromas are used in a very similar manner with an exception in some areas, like F#/Gb and when they use the C chroma most. For Taeko Onuki, this is in the beginning of the song, whereas for Whitney's, it is mostly in the end of the song. The F chroma is also used more. Overall, it seems like both of the songs seem very homogeneous in terms of pitch classes. There is not a lot of variation neither inter-, nor intra-song. Which is quite representative as the segments in the songs are very stereotypical with verses, choruses and repetition of these segments. Thus, in this instance the chromagrams do not particularly gain any insights in the differences in features. However, it does tell us that they are (can be) quite similar.  

On the other hand, it should be mentioned that for Taeko's song there is a good minute and a half dedicated to a rather simplistic guitar solo. However, based on this chromagram, this is not necessarily the case as they seem very homogenous. So perhaps, chromagrams does not necessarily work very well for these items in my corpus. 


Keys & Chords {.storyboard}
=======================================


```{r, include=FALSE}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )
```

### Carnaval - Taeko Onuki

```{r carnacal analysis}
carnaval <-
  get_tidy_audio_analysis("5GRcazsYepend8bMUKCTgW") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )
```

```{r carnaval chordogram}
carnaval |> 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if desired
    method = "manhattan",  # Try different distance metrics
    norm = "euclidean"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```

***

Here is a chordogram of the song "CARNAVAL" by Taeko Onuki. It is a song that has clear sound in terms of synthesizers, and the chordograms should be able to represent this accurately. Chordograms show similarity between bars and the chords inn the song, meaning that the darker the band the more likely this chord was played at this time in the song. However, many of the bands seem to be dark - which I am not entirely sure is representative. To give the benefit of the doubt, there are many parts of the song that the synthesizer has solos. 

In the bands many of them are dark around the major 7th chords, however also the minor 7th chords are dark. Especially around the end of the song. This could be in accordance to sources where it is indicated that CP employs these types of chords. 

In the next slide, a chordogram of the song "You're Still My Man" by Whitney Houston will be presented and compared. 


### You're Still My Man

```{r still my man analysis}
still_man <-
  get_tidy_audio_analysis("2csRQWI7A2FjLYBQBiAIoi") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )
```

```{r still my man chordogram}
still_man |> 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if desired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```

***

The song is a very calm love song, also equipped with a synthesizer, like the previous song. This should be a good way to compare the two genres. 
Here the bands are very dark around the same chords as the CP song. Both the major and minor 7th bands are dark throughout the song. "You're Still My Man" also has two modulations. This is represented well in the chordogram. 

However, in terms of differences between CP and UP there aren't any between these songs at least. 


Self-Similarity Matrices
======================================

Column {data-width=600}
--------------------------------------

### Self-Similarity Matrices of Material Girl and Mayonaka no Door/Stay With Me

```{r material girl_analysis}
mat_girl <-
  get_tidy_audio_analysis("7bkyXSi4GtVfD7itZRUR3e") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )
```

```{r staywme_analysis}
stayMe <-
  get_tidy_audio_analysis("2BHj31ufdEqVK5CkYDp9mA") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )
```



```{r}
bind_rows(
  mutate(compmus_self_similarity(stayMe, pitches, "cosine"), type = "Chroma", song = "Stay with Me"),
  mutate(compmus_self_similarity(stayMe, timbre, "cosine"), type = "Timbre", song = "Stay with Me"),
  mutate(compmus_self_similarity(mat_girl, pitches, "cosine"), type = "Chroma", song = "Material Girl"),
  mutate(compmus_self_similarity(mat_girl, timbre, "cosine"), type = "Timbre", song = "Material Girl"),
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(song~type) +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(title = "Self-Similarity Matrices", x = "", y = "")
```

Column {data-width=400}
--------------------------------------

### Analysis 

Here, two sets of self-similarity matrices in both chroma and timbre are compared between playlists in my corpus. These are the songs "Mayonaka no door / Stay With Me" by Miki Matsubara and "Material Girl" by Madonna. These were chosen as they are quite typical songs in my corpus. As ne can see, there are distinct differences between the two songs in both timbre and chroma.  

From the borderline black diagonal patterns in the chroma SSM one can see that the CP song definitely has more repetitive patterns in pitch classes than the UP one. There are novel items introduced, as one can see by the bright colors creating a checkerboard pattern, but this is more present in "Material Girl". The latter, according to the chroma SSM, shows less repetitions in pitch classes and more novel stuff. This is especially apparent at around 200 seconds in the song, where a brightly colored band shows up, indicating it is entirely novel. This indicates that CP is fond of repetitive pitch classes, perhaps even more than UP.  

However, if one looks at the timbre based SMMs, the story is entirely different. Here, there are many instances in the CP song where timbre has novelty at many different parts of the song as well. This is highly contrasted in the UP song, as one can see there are a lot of repetition in timbre and way less than CP. This supports the notion of the classifier that timbre might be the thing that really distinguishes CP from UP. 


Conclusions/Summary *UPDATED*
==================================

### Differences in City Pop vs US Pop 

Throughout this 




