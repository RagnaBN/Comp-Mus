---
title: "City Pop and Pop, the same despite being a world apart from one another?"
author: Ragna Nilsen
output: 
  flexdashboard::flex_dashboard:
    orientation: rows
    vertical_layout: fill
---

```{r setup, include=FALSE}
library(tidyverse)
library(tidymodels)
library(ggdendro)
library(spotifyr)
library(ggplot2)
library(flexdashboard)
library(plotly)
library(compmus)
  get_conf_mat <- function(fit) {
  outcome <- .get_tune_outcome_names(fit)
  fit |> 
    collect_predictions() |> 
    conf_mat(truth = outcome, estimate = .pred_class)
}  

get_pr <- function(fit) {
  fit |> 
    conf_mat_resampled() |> 
    group_by(Prediction) |> mutate(precision = Freq / sum(Freq)) |> 
    group_by(Truth) |> mutate(recall = Freq / sum(Freq)) |> 
    ungroup() |> filter(Prediction == Truth) |> 
    select(class = Prediction, precision, recall)
} 
library(knitr)
library(kknn)
library(heatmaply)
```

```{r load data, include=FALSE}
cityPopPlaylist <- get_playlist_audio_features("", "7LNMKu5boirXk30CB0mh7W")
popPlaylist <- get_playlist_audio_features("", "64h8THGlWTBkbE7sqhRV4V")
```

```{r bind data, include=FALSE}
comparison <- 
    bind_rows(
      cityPopPlaylist |> mutate(category = "City Pop"),
      popPlaylist |> mutate(category = "US Pop")
    )
```

Introduction
=======================================

 -PLEASE DO NOT SHOW THIS PORTFOLIO IN CLASS-   
    

I am comparing two playlists consisting of three artists, one Japanese group and one US playlist. The Japanese group consist of the artists Taeko Onuki, Miki Matsubara and Anri. The US counterparts are the artists Michael Jackson, Whitney Houston and Madonna. I chose these corpora because I want to explore whether there are distinct differences between the genre of (city) pop as it was in Japan in the 80's vs the pop that was popular in the western world in the same decade. Japanese city pop was influenced by western music, so I expect there to be many similarities in use of sound, instruments and type of rhythms. However, an aspect I am particularly interested whether there is a difference is the prevalence of bass, and rhythms. It is also interesting to see whether there are differences in other aspects like "supplementary" sounds. However, I am unsure to what extent they are different.  
    
As I have chosen three artists to represent their own (variety) of genres, there might be nuances and representations I am missing. Taeko Onuki, Miki Matsubara and Anri were chosen due to their popularity on Spotify (the amount of general listeners as well as listens to their tracks). I also have to mention that there were personal selections. The same method was done in choosing the western counterparts. However, the genre(s) is (are) very broad, despite its popularity, and some varieties might have been overlooked. However, their popularity is a strength. 
  

Typical, and popular, tracks from the Japanese playlist are:

  - "Mayonaka no Door / Stay With Me" - Miki Matsubara
  - "4:00A.M." - Taeko Onuki
  - "Remember Summer Days" - Anri  

These songs are typical in the sense that there are prominent use of basslines and clear rhythms, and have many "layers" to them. 

The western counterparts have typical tracks like: 

  - "Thriller" - Michael Jackson
  - "I Wanna Dance with Somebody (Who Loves Me)"
  - "Material Girl" - Madonna  
  
These last three tracks especially has the typical and distinct features of pop of the 80's, namely the sharp drums and the heavily synthesized piano sounds and, what I think, an almost like a "dreamy" sound to them. 
  
Atypical songs can include: 

  - "Billie Jean" by Michael Jackson
  - "横顔" by Taeko Onuki

This was an interesting find that proved to be quite interesting in the 

***

<iframe src="https://open.spotify.com/embed/playlist/4xnBi0S21rFHhhK0iGHpiS?utm_source=generator" width="100%" height="380" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture">

</iframe>


<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/7LNMKu5boirXk30CB0mh7W?utm_source=generator" width="100%" height="352" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>



Classification *NEW* {.storyboard}
======================================

### Confusion matrix 
```{r}
cPop <- get_playlist_audio_features("spotify", "7LNMKu5boirXk30CB0mh7W")
uPop <- get_playlist_audio_features("spotify", "64h8THGlWTBkbE7sqhRV4V")
Pop <-
  bind_rows(
    cPop |> mutate(playlist = "City Pop") |> slice_head(n = 31),
    uPop |> mutate(playlist = "U.S. Pop") |> slice_head(n = 31),
  ) |> 
  add_audio_analysis()
```

```{r}
pop_features <-
  Pop |>  # For your portfolio, change this to the name of your corpus.
  mutate(
    playlist = factor(playlist),
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(
        segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean",
      )
  ) |>
  mutate(pitches = map(pitches, compmus_normalise, "clr")) |>
  mutate_at(vars(pitches, timbre), map, bind_rows) |>
  unnest(cols = c(pitches, timbre))
```

```{r}
pop_recipe <-
  recipe(
    playlist ~
      danceability +
      energy +
      loudness +
      speechiness +
      acousticness +
      instrumentalness +
      liveness +
      valence +
      tempo +
      duration +
      C + `C#|Db` + D + `D#|Eb` +
      E + `F` + `F#|Gb` + G +
      `G#|Ab` + A + `A#|Bb` + B +
      c01 + c02 + c03 + c04 + c05 + c06 +
      c07 + c08 + c09 + c10 + c11 + c12,
    data = pop_features           # Use the same name as the previous block.
  ) |>
  step_center(all_predictors()) |>
  step_scale(all_predictors())      # Converts to z-scores.
  # step_range(all_predictors())    # Sets range to [0, 1].
```

```{r}
pop_cv <- pop_features |> vfold_cv(5)
```

```{r}
knn_model <-
  nearest_neighbor(neighbors = 1) |>
  set_mode("classification") |> 
  set_engine("kknn")
pop_knn <- 
  workflow() |> 
  add_recipe(pop_recipe) |> 
  add_model(knn_model) |> 
  fit_resamples(pop_cv, control = control_resamples(save_pred = TRUE))
```

```{r, include=FALSE}
pop_knn |> get_conf_mat()
```


```{r}
pop_knn |> get_conf_mat() |> autoplot(type = "heatmap")
```

*** 

```{r}
pop_knn |> get_pr()
```

  
In order to compute the model, I did capped the playlists at 31 songs in each group. The accuracy of the model as of right now is by the use of the formula *(TP+TN)/total* according to [this](https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/) website is (24+24)/62 = 0.77.., thus according to various sources [like this one](https://www.obviously.ai/post/machine-learning-model-performance) it is a relatively good accuracy while at the same time being within a realistic interval.

By this, I can assume that the model is actually very good at classifying exactly what City Pop and what U.S Pop is. In the next header, I will see what kind of labels were the most important in the classification of these playlists, and whether my visualizations from the earlier weeks were any sort of effective. 


### None of the features I used in visualizations were effective!


```{r}
forest_model <-
  rand_forest() |>
  set_mode("classification") |> 
  set_engine("ranger", importance = "impurity")
pop_forest <- 
  workflow() |> 
  add_recipe(pop_recipe) |> 
  add_model(forest_model) |> 
  fit_resamples(
    pop_cv, 
    control = control_resamples(save_pred = TRUE)
  )
```

```{r}
workflow() |> 
  add_recipe(pop_recipe) |> 
  add_model(forest_model) |> 
  fit(pop_features) |> 
  pluck("fit", "fit", "fit") |>
  ranger::importance() |> 
  enframe() |> 
  mutate(name = fct_reorder(name, value)) |> 
  ggplot(aes(name, value)) + 
  geom_col() + 
  coord_flip() +
  theme_minimal() +
  labs(x = NULL, y = "Importance")
```

***

```{r}
pop_forest |> get_pr()
```
  
  
```{r}
pop_forest |> get_conf_mat()
```

I hope this is the right way of getting the confusion matrix out of the random forest model. If I use the same formula as in the previous slide, i will get an accuracy of 0.77..., just as the last one. 

Regarding the feature selection, it is shown in the graph that the timbre vector c11, followed by the track level feature loudness, followed yet again by timbre vector c1. This is quite humorous as it was said in one of the lectures that this timbre vector is the rough equivalent to loudness. After those, primarily timbre features are the ones that are of the most importance. This is quite interesting, as I did mention at the very start of the course (without any knowledge of the terms of music) said that City Pop has a lot of layers to them. This was meant as "there is a lot going on" and it sounds different. 

For the final portfolio, I will use these features in order to improve the existing visualizations I have (and I have the ability to remove some of them). Yay!


Tempi {.storyboard}
======================================

### Histogram of tempi 

```{r}
comparison |> 
 ggplot(
    aes(
      x = tempo, fill = category
      )
    ) + 
  geom_histogram(
    binwidth = 5, 
    position = position_dodge(0.3)
    ) + 
  facet_wrap(~category) + 
  labs(
    y = "Amount of Songs", 
    x = "Tempo", 
    title = "Tempo distribution between City Pop and Pop", 
    citation = "Data: Personal Playlists", 
    fill = "Genre")
```

***  

Here is the distribution of tempi within my corpus, divided between the Japanese group and the U.S. group. As the graphs show, there are distinct differences in which tempo is preferred in the western world - which one can see is around 120 bpm. Interestingly, this corresponds well to the research that has been conducted where 120-125 bpm seems to be a natural tempo for humans and a link between the natural tempo and bpm in music has been found. However, according to the corpus I am using, this is not the case for the eastern world. The distribution here is more even, with more songs being in the range of 100-135 bpm. However, there are more songs that lie around the 105 bpm mark. 

Aside from the obvious archetypes in this distribution, there are two outliers from each group. These are the songs "A HOPE FROM SAD STREET" by Anri, and "Love Is a Contact Sport" by Whitney Houston. They each have tempi that lie around 180 and 175 bpm respectively. 


### Billie Jean - our atypical song 

```{r}
knitr::include_graphics("billiejean.png")
```

***  

Here is the tempogram of the indentified outlier from the graph of energy's effect on danceability, where the energy is low, but danceability is high. This is a completely standard tempogram, in terms of the overall execution of the formulations on this piece as there are no disturbances or anything that went wrong. "Billie Jean" is a very stereotypical song in this regard, with a bpm of approximately 120 and very steady beat, rhythm and tempo that do not seem to change. This is probably a part of the reason the overall danceability of this piece is very high, in spite that its energy is very low. 


### A Hope From Sad Street - An Outlier in Distribution

```{r}
knitr::include_graphics("sad-street.png")
```

***  

Providing the distribution of tempi in the form of a histogram, there was a couple outliers in both genres. "A HOPE FROM SAD STREET" by Anri is one of them. Here, in comparison to "Billie Jean" there were apparantly some issues when generating the tempo, as there are multiple yellow lines that flicker across the entire piece. However, as one can tell, it was not a complete failure, as it was able to generate not one, but two tempo octaves. It is not as stable as the previous piece, but the lines are clearly there. 

This flickering could be attributed to the way this song is realized, in that there are many layers of sound, there are the bassline, but there are also a layer of trumpet as well as a choir and points where the song is stopped and picked up again. The bassline often is replaced by piano, and guitar solos - before returning to the "status quo". As far as I can tell, these instances are represented in the points in the tempogram. 


### Love Is a Contact Sport - Western Outlier in Distribution

```{r}
knitr::include_graphics("contact-sport.png")
```

***

In comparison to Anri's song, "Love is a Contact Sport" by Whitney Houston does not have any issues at all, as clearly shown by the image. The song is also realized very clearly, with a clear tempo throughout the entire song. This song was also one of the outliers in the distribution chart, and was the western counterpart to Anri's. Similarly to the aforementioned song, this has two tempo-octaves. 

### Interesting find - Cat's Eye by Anri

```{r}
knitr::include_graphics("catseye.png")
```

***

Once again, here is a song by Anri, called "CAT'S EYE - (NEW TAKE)". I wanted to include this, due to the algorithm being able to generate the tempo very clearly in the beginning but when the song has reached around 130 seconds, there is a clear window where it was not able to generate the tempo. This is very clear if you listen to the song, as there is a moment around that time where the bass guitar has a solo. To me, it feels quicker than the song itself, which it has been able to show in some regard - however, this shift was clearly something the tempogram could not handle and was interesting to me. 


Keys & Chords {.storyboard}
=======================================

### Histogram of keys
```{r}
comparison |> 
 ggplot(
    aes(
      x = key, fill = category
      )
    ) + 
  geom_histogram(
    binwidth = 0.5, 
    position = position_dodge(0.3)
    ) + 
  labs(
    y = "Amount of Songs", 
    x = "Key", 
    title = "Key distribution between City Pop and Pop", 
    citation = "Data: Personal Playlists", 
    fill = "Genre")
```

***

As one can see from the graph here, the distribution of keys between each song in each of my corpus group are, overall, quite evenly distributed. However, one can tell that different groups often prefer different keys.   City Pop seems to prefer keys that are in C, D, F, and G for the most part, whereas U.S. Pop, while sharing a decent amount of songs in C as well, seem to also prefer D#, A, and B - unlike City Pop. It should be noted however, that there are a few songs more in the city pop group than the U.S. one, so it can skew the distributions. 



```{r, include=FALSE}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )
```


### Mayonaka no Door/Stay With Me     City Pop
```{r}
Stay <-
  get_tidy_audio_analysis("2BHj31ufdEqVK5CkYDp9mA") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )
```

```{r}
Stay |> 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if desired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```


### 4:00A.M.     City Pop

```{r}
four_am <-
  get_tidy_audio_analysis("0zoGVO4bQXG8U6ChKwNgeg") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )
```

```{r}
four_am |> 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if desired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```


### Remember Summer Days     City Pop

```{r}
remember <-
  get_tidy_audio_analysis("1qUo7d5lAOclNVbTUY0A2R") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )
```

```{r}
remember |> 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if desired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```


### I Wanna Dance With Somebody (Who Loves Me)     Pop

```{r}
wanna_dance <-
  get_tidy_audio_analysis("2tUBqZG2AbRi7Q0BIrVrEj") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )
```

```{r}
wanna_dance |> 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if desired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```


### Material Girl     Pop

```{r}
material_girl <-
  get_tidy_audio_analysis("7bkyXSi4GtVfD7itZRUR3e") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )
```

```{r}
material_girl |> 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if desired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```


### Thriller     Pop

```{r}
thriller <-
  get_tidy_audio_analysis("2LlQb7Uoj1kKyGhlkBf9aC") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )
```

```{r}
thriller |> 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if desired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```


Chromagram of the outlier *UPDATED*
====================================

### Billie Jean - Michael Jackson {data-width=700}
```{r}
bjean <- 
    get_tidy_audio_analysis("7J1uxwnxfQLu4APicE5Rnj") |>
    select(segments) |>
    unnest(segments) |>
    select(start, duration, pitches)

bjean |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()
```

### Commentary {data-width=300}

"Billie Jean" was listed as an outlier for the Pop group in the graph of whether energy has an effect on danceability, in the sense that Spotify notices this as a non energetic song, while having an extremely high danceability. As one can see in this chromagram, the song "Billie Jean" has several areas where the magnitude is over 0.75. A couple of patterns that arise are the use of the D and C#/Db at before 100 and 200 seconds. They form an almost pyramid shape. However, despite this, one can see that it is a very energetic song, by the use of the chromas - despite being a very slow and unenergetic song. 


Graphs {.storyboard}
==========================================
 

### Distribution of energy between City Pop and US Pop  

```{r interactive plot}
energy_hist <- comparison |> 
 ggplot(
    aes(
      x = energy, fill = category
      )
    ) + 
  geom_histogram(
    binwidth = 0.1, 
    position = position_dodge(0.3)
    ) + 
  labs(
    y = "Amount of Songs", 
    x = "Energy", 
    title = "Distribution of Energy between City Pop and Pop", 
    citation = "Data: Personal Playlists")

ggplotly(energy_hist)
```


***

Here are plots to give insight in the general tempos effect on energy in the groups City Pop and US Pop. In the first histogram plot, one can see that the distribution of energy is more even for pop than for city pop. It says that it counted 15 of the data in my playlist for City Pop to have an energy of a little over 0.6. 
 

### Tempo and Energy *UPDATED*


```{r}
comparison |>
  mutate(
    mode = ifelse(mode == 0, "Minor", "Major")
    ) |>
  ggplot(
    aes(
      x = tempo, 
      y = energy, 
      color = category,
      size = loudness
      )
    ) + 
    geom_point(
       alpha = 0.2
    ) + 
    geom_jitter() + 
    facet_wrap(
      ~category
      ) +
    labs(
      title = "The effect of tempo on energy between groups", 
      x = "Tempo", 
      y = "Energy",
      size = "Loudness", 
      color = "Genre"
    )
```



### Danceability on Valence *UPDATED*


```{r interactive }
comparison |>
  ggplot(
    aes(
      x = danceability, 
      y = valence, 
      color = category, 
      size = loudness
      )
    ) + 
    geom_point(
      ) + 
    geom_jitter() +
    facet_wrap(~category) +
    labs(
      title = "The effect of danceability on valence for City Pop and US Pop", 
      size = "Loudness", 
      x = "Danceability", 
      y = "Valence")
```

### Energy on danceability *NEW*

```{r}
comparison |>
  ggplot(
    aes(
      x = energy, 
      y = danceability, 
      color = category, 
      size = tempo
      )
    ) + 
    geom_point() + 
    geom_smooth() +
    facet_wrap(~category) +
    labs(
      title = "The effect of danceability on valence for City Pop and US Pop", 
      size = "Tempo", 
      x = "Energy", 
      y = "Danceability")
```

***

Here is a plot of the effect of energy on danceability, with size of the plots as well as the band around the line indicating the tempo of the songs. One can see that there is more of a linear trend with US Pop, indicating that up until around energy of 0.5 there seems to be a correlation between energy and danceability. Tempo, however, do seem to have no pattern at first glance. Whereas for the City Pop playlist, there seem to be a slight curve in the beginning of the graph, but overall there is a very even trend of the effect of energy on danceability. At first glance, there also seem to be no indication that there is a trend for tempo.  

However, once can tell that U.S. Pop seem to have more of a positive linear correlation between energy and danceability - more than its eastern counterpart in any case.

In both groups, there are two outliers that especially draw one's eye - which is "Billie Jean" by Michael Jackson, and "横顔" by Taeko Onuki. 


Conclusions/Summary *UPDATED*
==================================

### Differences in City Pop vs US Pop *HAS NOT CHANGED* 

So far we can see that the differences between US Pop and City Pop, based on the corpus I am using, is that energy definitely seem to have an effect on danceability in US pop, up until a certain level. However, there are many outliers that can skew this, which will be identified shortly. Furthermore, it does not seem that tempo has a definite pattern on either energy or danceability. 

Furthermore, another difference is that danceability seem to have more of an effect on the positive valence in both genres. However, one can see that there are more songs in US pop that are more danceable and that are happier than City Pop.  

This makes sense, given the fact that according to the histogram, none of the city pop songs go above a certain threshold of valence, in comparison to US pop. This could indicate that city pop is generally less "happy".  

But one outlier I want to talk about in particular is the one track, in the energy, danceability and tempo plot, where the energy is quite low in comparison to other tracks, but the danceability is one of the highest in the group. While I am not sure how to point this out in the plot, I have managed to identify it as the track "Billie Jean" by Michael Jackson. A seperate chromagram has been made in order to account for this. 


### Homework Comments *UPDATED*

I seem to have been able to provide analyses on the tempi-histogram as well as some outliers in different regards for this homework. Please give some feedback in regards to whether there are some analytic parts that are missing or might be good to add. Thank you!


