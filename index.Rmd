---
title: "City Pop and Pop, the same despite being a world apart from one another?"
author: Ragna Nilsen
output: 
  flexdashboard::flex_dashboard:
    orientation: rows
    vertical_layout: fill
---

```{r setup, include=FALSE}
library(tidyverse)
library(spotifyr)
library(ggplot2)
library(flexdashboard)
library(plotly)
library(compmus)
library(knitr)
```

```{r load data, include=FALSE}
cityPopPlaylist <- get_playlist_audio_features("", "7LNMKu5boirXk30CB0mh7W")
popPlaylist <- get_playlist_audio_features("", "64h8THGlWTBkbE7sqhRV4V")
```

```{r bind data, include=FALSE}
comparison <- 
    bind_rows(
      cityPopPlaylist |> mutate(category = "City Pop"),
      popPlaylist |> mutate(category = "US Pop")
    )
```


Tempi *NEW* {.storyboard}
======================================

### Histogram of tempi 

```{r}
comparison |> 
 ggplot(
    aes(
      x = tempo, fill = category
      )
    ) + 
  geom_histogram(
    binwidth = 5, 
    position = position_dodge(0.3)
    ) + 
  facet_wrap(~category) + 
  labs(
    y = "Amount of Songs", 
    x = "Tempo", 
    title = "Tempo distribution between City Pop and Pop", 
    citation = "Data: Personal Playlists", 
    fill = "Genre")
```

***  

Here is the distribution of tempi within my corpus, divided between the Japanese group and the U.S. group. As the graphs show, there are distinct differences in which tempo is preferred in the western world - which one can see is around 120 bpm. Interestingly, this corresponds well to the research that has been conducted where 120-125 bpm seems to be a natural tempo for humans and a link between the natural tempo and bpm in music has been found. However, according to the corpus I am using, this is not the case for the eastern world. The distribution here is more even, with more songs being in the range of 100-135 bpm. However, there are more songs that lie around the 105 bpm mark. 

Aside from the obvious archetypes in this distribution, there are two outliers from each group. These are the songs "A HOPE FROM SAD STREET" by Anri, and "Love Is a Contact Sport" by Whitney Houston. They each have tempi that lie around 180 and 175 bpm respectively. 


### Billie Jean - our atypical song 

```{r}
knitr::include_graphics("billiejean.png")
```

***  

Here is the tempogram of 


### A Hope From Sad Street - An Outlier in Distribution

```{r}
knitr::include_graphics("sad-street.png")
```

### Love Is a Contact Sport - Western Outlier in Distribution

```{r}
knitr::include_graphics("contact-sport.png")
```

### Interesting find - Cat's Eye by Anri

```{r}
knitr::include_graphics("catseye.png")
```

***



Keys & Chords {.storyboard}
=======================================

### Histogram of keys
```{r}
comparison |> 
 ggplot(
    aes(
      x = key, fill = category
      )
    ) + 
  geom_histogram(
    binwidth = 0.5, 
    position = position_dodge(0.3)
    ) + 
  labs(
    y = "Amount of Songs", 
    x = "Key", 
    title = "Key distribution between City Pop and Pop", 
    citation = "Data: Personal Playlists", 
    fill = "Genre")
```

***

As one can see from the graph here, the distribution of keys between each song in each of my corpus group are, overall, quite evenly distributed. However, one can tell that different groups often prefer different keys.   City Pop seems to prefer keys that are in C, D, F, and G for the most part, whereas U.S. Pop, while sharing a decent amount of songs in C as well, seem to also prefer D#, A, and B - unlike City Pop. It should be noted however, that there are a few songs more in the city pop group than the U.S. one, so it can skew the distributions. 


Chordograms of Archetypes {.storyboard}
=======================================

```{r, include=FALSE}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )
```


### Mayonaka no Door/Stay With Me     City Pop
```{r}
Stay <-
  get_tidy_audio_analysis("2BHj31ufdEqVK5CkYDp9mA") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )
```

```{r}
Stay |> 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if desired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```


### 4:00A.M.     City Pop

```{r}
four_am <-
  get_tidy_audio_analysis("0zoGVO4bQXG8U6ChKwNgeg") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )
```

```{r}
four_am |> 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if desired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```


### Remember Summer Days     City Pop

```{r}
remember <-
  get_tidy_audio_analysis("1qUo7d5lAOclNVbTUY0A2R") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )
```

```{r}
remember |> 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if desired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```


### I Wanna Dance With Somebody (Who Loves Me)     Pop

```{r}
wanna_dance <-
  get_tidy_audio_analysis("2tUBqZG2AbRi7Q0BIrVrEj") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )
```

```{r}
wanna_dance |> 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if desired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```


### Material Girl     Pop

```{r}
material_girl <-
  get_tidy_audio_analysis("7bkyXSi4GtVfD7itZRUR3e") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )
```

```{r}
material_girl |> 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if desired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```


### Thriller     Pop

```{r}
thriller <-
  get_tidy_audio_analysis("2LlQb7Uoj1kKyGhlkBf9aC") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )
```

```{r}
thriller |> 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if desired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```


Introduction
=======================================

 -PLEASE DO NOT SHOW THIS PORTFOLIO IN CLASS-   
    
 
I am comparing two playlists consisting of three artists, one Japanese group and one US playlist. The Japanese group consist of the artists Taeko Onuki, Miki Matsubara and Anri. The US counterparts are the artists Michael Jackson, Whitney Houston and Madonna. I chose these corpora because I want to explore whether there are distinct differences between the genre of (city) pop as it was in Japan in the 80's vs the pop that was popular in the western world in the same decade. Japanese city pop was influenced by western music, so I expect there to be many similarities in use of sound, instruments and type of rhythms. However, an aspect I am particularly interested whether there is a difference is the prevalence of bass, and rhythms. It is also interesting to see whether there are differences in other aspects like "supplementary" sounds. However, I am unsure to what extent they are different.  
    
As I have chosen three artists to represent their own (variety) of genres, there might be nuances and representations I am missing. Taeko Onuki, Miki Matsubara and Anri were chosen due to their popularity on Spotify (the amount of general listeners as well as listens to their tracks). I also have to mention that there were personal selections. The same method was done in choosing the western counterparts. However, the genre(s) is (are) very broad, despite its popularity, and some varieties might have been overlooked. However, their popularity is a strength. 
  

Typical, and popular, tracks from the Japanese playlist are:

  - "Mayonaka no Door / Stay With Me" - Miki Matsubara
  - "4:00A.M." - Taeko Onuki
  - "Remember Summer Days" - Anri  

These songs are typical in the sense that there are prominent use of basslines and clear rhythms, and have many "layers" to them. 

The western counterparts have typical tracks like: 

  - "Thriller" - Michael Jackson
  - "I Wanna Dance with Somebody (Who Loves Me)"
  - "Material Girl" - Madonna  
  
These last three tracks especially has the typical and distinct features of pop of the 80's, namely the sharp drums and the heavily synthesized piano sounds and, what I think, an almost like a "dreamy" sound to them. 
  
Atypical songs can include: 

  - "Billie Jean" by Michael Jackson
  - "横顔" by Taeko Onuki

This was an interesting find that proved to be quite interesting in the 



Chromagram of the outlier *UPDATED*
====================================

### Billie Jean - Michael Jackson {data-width=700}
```{r}
bjean <- 
    get_tidy_audio_analysis("7J1uxwnxfQLu4APicE5Rnj") |>
    select(segments) |>
    unnest(segments) |>
    select(start, duration, pitches)

bjean |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()
```

### Commentary {data-width=300}

"Billie Jean" was listed as an outlier for the Pop group in the graph of whether energy has an effect on danceability, in the sense that Spotify notices this as a non energetic song, while having an extremely high danceability. As one can see in this chromagram, the song "Billie Jean" has several areas where the magnitude is over 0.75. A couple of patterns that arise are the use of the D and C#/Db at before 100 and 200 seconds. They form an almost pyramid shape. However, despite this, one can see that it is a very energetic song, by the use of the chromas - despite being a very slow and unenergetic song. 


Graphs {.storyboard}
==========================================
 

### Distribution of energy between City Pop and US Pop  

```{r interactive plot}
energy_hist <- comparison |> 
 ggplot(
    aes(
      x = energy, fill = category
      )
    ) + 
  geom_histogram(
    binwidth = 0.1, 
    position = position_dodge(0.3)
    ) + 
  labs(
    y = "Amount of Songs", 
    x = "Energy", 
    title = "Distribution of Energy between City Pop and Pop", 
    citation = "Data: Personal Playlists")

ggplotly(energy_hist)
```

Here are plots to give insight in the general tempos effect on energy in the groups City Pop and US Pop. In the first histogram plot, one can see that the distribution of energy is more even for pop than for city pop. It says that it counted 15 of the data in my playlist for City Pop to have an energy of a little over 0.6. 
 

### Tempo and Energy *UPDATED*


```{r}
comparison |>
  mutate(
    mode = ifelse(mode == 0, "Minor", "Major")
    ) |>
  ggplot(
    aes(
      x = tempo, 
      y = energy, 
      color = category,
      size = loudness
      )
    ) + 
    geom_point(
       alpha = 0.2
    ) + 
    geom_jitter() + 
    facet_wrap(
      ~category
      ) +
    labs(
      title = "The effect of tempo on energy between groups", 
      x = "Tempo", 
      y = "Energy",
      size = "Loudness", 
      color = "Genre"
    )
```



### Danceability on Valence *UPDATED*


```{r interactive }
comparison |>
  ggplot(
    aes(
      x = danceability, 
      y = valence, 
      color = category, 
      size = loudness
      )
    ) + 
    geom_point(
      ) + 
    geom_jitter() +
    facet_wrap(~category) +
    labs(
      title = "The effect of danceability on valence for City Pop and US Pop", 
      size = "Loudness", 
      x = "Danceability", 
      y = "Valence")
```

### Energy on danceability *NEW*

```{r}
comparison |>
  ggplot(
    aes(
      x = energy, 
      y = danceability, 
      color = category, 
      size = tempo
      )
    ) + 
    geom_point() + 
    geom_smooth() +
    facet_wrap(~category) +
    labs(
      title = "The effect of danceability on valence for City Pop and US Pop", 
      size = "Tempo", 
      x = "Energy", 
      y = "Danceability")
```

***

Here is a plot of the effect of energy on danceability, with size of the plots as well as the band around the line indicating the tempo of the songs. One can see that there is more of a linear trend with US Pop, indicating that up until around energy of 0.5 there seems to be a correlation between energy and danceability. Tempo, however, do seem to have no pattern at first glance. Whereas for the City Pop playlist, there seem to be a slight curve in the beginning of the graph, but overall there is a very even trend of the effect of energy on danceability. At first glance, there also seem to be no indication that there is a trend for tempo.  

However, once can tell that U.S. Pop seem to have more of a positive linear correlation between energy and danceability - more than its eastern counterpart in any case.

In both groups, there are two outliers that especially draw one's eye - which is the 

Conclusions/Summary *UPDATED*
==================================

### Differences in City Pop vs US Pop 

So far we can see that the differences between US Pop and City Pop, based on the corpus I am using, is that energy definitely seem to have an effect on danceability in US pop, up until a certain level. However, there are many outliers that can skew this, which will be identified shortly. Furthermore, it does not seem that tempo has a definite pattern on either energy or danceability. 

Furthermore, another difference is that danceability seem to have more of an effect on the positive valence in both genres. However, one can see that there are more songs in US pop that are more danceable and that are happier than City Pop.  

This makes sense, given the fact that according to the histogram, none of the city pop songs go above a certain threshold of valence, in comparison to US pop. This could indicate that city pop is generally less "happy".  

But one outlier I want to talk about in particular is the one track, in the energy, danceability and tempo plot, where the energy is quite low in comparison to other tracks, but the danceability is one of the highest in the group. While I am not sure how to point this out in the plot, I have managed to identify it as the track "Billie Jean" by Michael Jackson. A seperate chromagram has been made in order to account for this. 


### Homework Comments *UPDATED*

Due to illnesses that have lasted for exactly a week, I have not been able to do much commentary on the new additions to my portfolio aside from just doing the actual homework. This is why you see that "Homework 9" is missing. I hope to add more when I have the time, as well as feeling a bit better.  

Furthermore, I seem to have misread the assignment for today, so my analysis part of this homework is quite sparse while having a lot of graphs. My excuses for this. 


